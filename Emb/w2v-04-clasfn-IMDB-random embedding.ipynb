{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b154103-ec70-4e04-8ad0-c2e2afe55e32",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "#### Word embeddings \n",
    "--------------------------\n",
    "\n",
    "- Objective\n",
    "\n",
    "    - build a binary classification model\n",
    "    - perform sentiment analysis on IMDB dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f734635-06ac-44cf-a623-7e568abb808c",
   "metadata": {},
   "source": [
    "**Data Download and Extraction:**\n",
    "\n",
    "Downloads a sentiment analysis dataset (IMDb reviews) from a specified URL.\n",
    "Extracts the dataset from the downloaded tar.gz file.\n",
    "\n",
    "**Data Preparation:**\n",
    "\n",
    "Creates directories for training and validation data.\n",
    "Loads the training data using text_dataset_from_directory from TensorFlow, splitting it into training and validation subsets.\n",
    "\n",
    "**Data Preprocessing and Optimization:**\n",
    "\n",
    "Defines the custom_standardization function to perform text preprocessing, converting text to lowercase and stripping HTML break tags.\n",
    "Uses the TextVectorization layer to normalize, split, and map strings to integers, adapting it to the training data.\n",
    "Sets up the AUTOTUNE constant and applies caching and prefetching to the training and validation datasets for optimized performance.\n",
    "\n",
    "**Model Definition:**\n",
    "\n",
    "Creates a text classification model using TensorFlow's Keras API.\n",
    "Comprises layers for text vectorization, embedding, global average pooling, and two dense layers for classification.\n",
    "Specifies the vocabulary size, sequence length, and embedding dimension.\n",
    "\n",
    "**Model Training:**\n",
    "\n",
    "Compiles and trains the defined model on the preprocessed training dataset.\n",
    "Utilizes the fit method with specified parameters such as training data, validation data, number of epochs, and callbacks.\n",
    "\n",
    "**TensorBoard Callback:**\n",
    "\n",
    "There is a reference to a tensorboard_callback, which suggests the usage of TensorBoard for model training visualization. However, the instantiation and definition of this callback are not provided in the provided code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d004f780-6eba-4dc0-bf4c-8e4be5d08dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ebf46-0955-4c0e-9fdb-e72c7e95ea3a",
   "metadata": {},
   "source": [
    "#### Download the IMDb Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f9b937-4ed9-44ee-bb6c-618d6c297f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 33s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# 82 MB file\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", \n",
    "                                  url,\n",
    "                                  untar       = True, \n",
    "                                  cache_dir   = r'D:\\AI-DATASETS\\02-MISC-large\\keras\\datasets',\n",
    "                                  cache_subdir= '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7359e98-c04b-469b-b06d-c0fa7051f3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9f93c7-c3d8-4b0e-bdc2-4c54b1a11237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AI-DATASETS\\\\02-MISC-large\\\\keras\\\\datasets\\\\aclImdb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5623f-776f-49ec-879a-4302664a6866",
   "metadata": {},
   "source": [
    "**train/ directory**\n",
    "\n",
    "- `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "126327bc-ac7c-48ee-a4f9-e91e1f5c41d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75268314-adf6-448e-9dbc-ae3124b0f56f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AI-DATASETS\\\\02-MISC-large\\\\keras\\\\datasets\\\\aclImdb\\\\train'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed687bce-2711-42fb-b3a3-58db934cd3f6",
   "metadata": {},
   "source": [
    "The train directory also has additional folders which should be removed before creating training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd497ae-6cce-4c62-a0d1-af99d836e29e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\AI-DATASETS\\\\02-MISC-large\\\\keras\\\\datasets\\\\aclImdb\\\\train\\\\unsup'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\shutil.py:603\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    601\u001b[0m         entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 603\u001b[0m     onerror(os\u001b[38;5;241m.\u001b[39mscandir, path, sys\u001b[38;5;241m.\u001b[39mexc_info())\n\u001b[0;32m    604\u001b[0m     entries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\shutil.py:600\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rmtree_unsafe\u001b[39m(path, onerror):\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 600\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(path) \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[0;32m    601\u001b[0m             entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\AI-DATASETS\\\\02-MISC-large\\\\keras\\\\datasets\\\\aclImdb\\\\train\\\\unsup'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb9fcf-19ca-4638-8049-434b16e29282",
   "metadata": {},
   "source": [
    "Next, create a `tf.data.Dataset` using `tf.keras.utils.text_dataset_from_directory`.\n",
    "\n",
    "Use the train directory to create both train and validation datasets with a split of 20% for validation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe6de88d-39b8-4d0f-a132-3b8e99c2ce2f",
   "metadata": {},
   "source": [
    "If your directory structure is:\n",
    "\n",
    "```\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_text_1.txt\n",
    "......a_text_2.txt\n",
    "...class_b/\n",
    "......b_text_1.txt\n",
    "......b_text_2.txt\n",
    "```\n",
    "\n",
    "Then calling `text_dataset_from_directory(main_directory,\n",
    "labels='inferred')` will return a `tf.data.Dataset` that yields batches of\n",
    "texts from the subdirectories `class_a` and `class_b`, together with labels\n",
    "0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c9a161-c0f4-46bc-9633-9db1ea58d5af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "CPU times: total: 4.11 s\n",
      "Wall time: 8.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 1024\n",
    "seed       = 123\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "                    train_dir, \n",
    "                    batch_size      = batch_size, \n",
    "                    validation_split= 0.2,\n",
    "                    subset          = 'training', \n",
    "                    seed            = seed)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "                    train_dir, \n",
    "                    batch_size      = batch_size, \n",
    "                    validation_split= 0.2,\n",
    "                    subset          = 'validation', \n",
    "                    seed            = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf132e-9d4b-4cdf-a2f7-11afa0e63281",
   "metadata": {},
   "source": [
    "Take a look at a few movie reviews and their labels (1: positive, 0: negative) from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3d4bff4-6828-4027-a533-ea2bf352d85c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b\"Wow. Some movies just leave me speechless. This was undeniably one of those movies. When I left the theatre, not a single word came to my mouth. All I had was an incredible urge to slam my head against the theatre wall to help me forget about the last hour and a half. Unfortunately, it didn't work. Honestly, this movie has nothing to recommend. The humor was at the first grade level, at best, the acting was overly silly, and the plot was astronomically far-fetched. I hearby pledge never to see an other movie starring Chris Kattan or any other cast-member of SNL.\"\n",
      "\n",
      "1 b'If any show in the last ten years deserves a 10, it is this rare gem. It allows us to escape back to a time when things were simpler and more fun. Filled with heart and laughs, this show keeps you laughing through the three decades of difference. The furniture was ugly, the clothes were colorful, and the even the drugs were tolerable. The hair was feathered, the music was accompanied by roller-skates, and in the words of Merle Haggard, \"a joint was a bad place to be\". Take a trip back to the greatest time in American history. Fall in love with characters and the feel good essence of the small town where people were nicer to each other. This classic is on television as much as \"Full House\". Don\\'t miss it, and always remember to \"Shake your groove thing!!!\"'\n",
      "\n",
      "CPU times: total: 16.8 s\n",
      "Wall time: 43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3312b2-b7c5-42f0-8fe4-36c1d476b80d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sets the variable AUTOTUNE to the special value tf.data.AUTOTUNE, \n",
    "# which is a constant used in TensorFlow to dynamically tune the performance of \n",
    "# operations based on the available resources.\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# caches the elements of the dataset in memory\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c4d38-8d58-4a9b-b560-4a841618b22d",
   "metadata": {},
   "source": [
    "#### how are we going to create embeddings\n",
    "\n",
    "Given review text : \"Some movies just leave me speechless.. any other cast-member of SNL\"\n",
    "\n",
    "1. first tokenize the text into words\n",
    "2. assign unique integer number (think like a code) to every word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a96e9-bc44-4ab2-83ea-6c38b82de376",
   "metadata": {},
   "source": [
    "#### Using the Embedding layer\n",
    "\n",
    "- The Embedding layer serves as a lookup table, associating integer indices with dense vectors that represent the embeddings of specific words.\n",
    "- It can be compared to a parameterized table where each word is assigned a unique dense vector.\n",
    "- The dimensionality or width of the embedding is a tunable parameter, allowing experimentation to find an optimal setting for a given problem.\n",
    "- Similar to adjusting the number of neurons in a Dense layer, modifying the embedding dimensionality enables fine-tuning for improved model performance.\n",
    "- Experimenting with different embedding dimensions helps determine the most effective representation of words in the context of a particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c523b85d-ea3a-4d62-a4d1-d202f396dd22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ANACONDA\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6294364-0fa9-4dd6-8a8f-d8720c35a755",
   "metadata": {},
   "source": [
    "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d0c1e72-23eb-483c-8f98-9a4522e14c53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03098005,  0.03636936, -0.03422924,  0.02029983,  0.00572123],\n",
       "       [ 0.02952042,  0.03769893,  0.04816072, -0.02836904,  0.01211315],\n",
       "       [ 0.02682513, -0.03596522, -0.03820059,  0.02709473, -0.04366157]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47f75d-f196-4c8f-8284-8be7708f0f85",
   "metadata": {},
   "source": [
    "**for text data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11992e-0277-4de5-9fce-d68580b44125",
   "metadata": {},
   "source": [
    "- For text or sequence-related problems, the Embedding layer in neural networks accepts a 2D tensor of integers with a shape of (`samples`, `sequence_length`).\n",
    "\n",
    "- Each entry in this tensor represents a sequence of integers, allowing the layer to handle variable-length sequences effectively.\n",
    "\n",
    "- Batches with different shapes, such as (32, 10) or (64, 15), can be fed into the Embedding layer, where 32 or 64 represents the number of sequences in the batch, and 10 or 15 is the length of each sequence.\n",
    "\n",
    "- The resulting tensor from the Embedding layer has one additional axis compared to the input. The embedding vectors are aligned along this new last axis.\n",
    "- If a batch with a shape of (2, 3) is passed to the Embedding layer, the output tensor will be of shape (2, 3, N), where N represents the dimensionality of the embedding space. \n",
    "- The embeddings for each integer in the input sequences are aligned along the new axis, preserving the sequence structure.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e0d7c6-8324-45d4-876b-d53c25197d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf791148-dbcc-4675-8f80-1e67f77696a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2], \n",
    "                                      [3, 4, 5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97024270-1d48-4342-b187-46678bdce98f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 5), dtype=float32, numpy=\n",
       "array([[[-0.01677575,  0.0234295 , -0.00909972,  0.03595456,  0.04958868],\n",
       "        [ 0.03098005,  0.03636936, -0.03422924,  0.02029983,  0.00572123],\n",
       "        [ 0.02952042,  0.03769893,  0.04816072, -0.02836904,  0.01211315]],\n",
       "\n",
       "       [[ 0.02682513, -0.03596522, -0.03820059,  0.02709473, -0.04366157],\n",
       "        [ 0.00437683,  0.0455189 , -0.04694527,  0.03287429,  0.00334846],\n",
       "        [-0.04294212,  0.00376356, -0.00407047,  0.00884806,  0.00669036]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710428fa-0f8f-4beb-b430-4f3b9039f231",
   "metadata": {},
   "source": [
    "- When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape (`samples`, `sequence_length`, `embedding_dimensionality`). \n",
    "- To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. \n",
    "- You could use an RNN, Attention, or pooling layer before passing it to a Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdae533-48d6-4014-b7f1-37c83c41a61b",
   "metadata": {},
   "source": [
    "#### Text preprocessing\n",
    "Next, define the dataset preprocessing steps required for your sentiment classification model. \n",
    "\n",
    "Initialize a `TextVectorization layer` with the desired parameters to vectorize movie reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d4d6b22-6ac6-4811-95a7-bfdaba1c6eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a64096a3-288f-4b90-8f12-4297b7689546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data\n",
    "train_texts = [\"This is a sample sentence.\", \n",
    "               \"Another example sentence.\", \n",
    "               \"TensorFlow is great!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab608003-d6c5-4b8b-a5d3-b321f91e9187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a TextVectorization layer\n",
    "text_vectorizer = TextVectorization(max_tokens            = 100, \n",
    "                                    output_mode           = 'int', \n",
    "                                    output_sequence_length= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e73d9ef8-82ee-4248-8215-c070cfffeec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ANACONDA\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adapt the layer to your training text data\n",
    "text_vectorizer.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a2f6d6f-0428-4480-a0a8-a4f10b3d4a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform input text into numerical vectors\n",
    "input_texts = [\"Sample sentence for testing.\", \"TensorFlow example.\"]\n",
    "numerical_vectors = text_vectorizer(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b620d3b-c51d-4ace-bda3-0421a2b98607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts:\n",
      "['This is a sample sentence.', 'Another example sentence.', 'TensorFlow is great!']\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Original texts:\")\n",
    "print(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fedf545f-b574-4655-89bd-92aa07c15e41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical vectors:\n",
      "[[6 2 1 1 0]\n",
      " [5 8 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNumerical vectors:\")\n",
    "print(numerical_vectors.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148bc74-cf1a-4577-8076-a35729a1b009",
   "metadata": {},
   "source": [
    "...back to code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5185769d-b716-4c88-90f2-263d52c003ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase     = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    \n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3376a7fe-c48c-44ab-8be8-79d5770209f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size      = 10000\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f883ddf1-1942-4a8f-8559-84a0402603ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. \n",
    "# Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "                        standardize           = custom_standardization,\n",
    "                        max_tokens            = vocab_size,\n",
    "                        output_mode           = 'int',\n",
    "                        output_sequence_length= sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b002dc1-5dbc-4e5e-8d90-0341028172bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.38 s\n",
      "Wall time: 5.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d753b64-a51a-4861-b9a8-154428fc7a0c",
   "metadata": {},
   "source": [
    "#### Constructing a Sentiment Classification Model\n",
    "\n",
    "- Utilize the Keras Sequential API to establish a sentiment classification model, specifically adopting a `Continuous Bag of Words` style.\n",
    "\n",
    "- The TextVectorization layer plays a crucial role in transforming strings into vocabulary indices. \n",
    "\n",
    "- After initializing vectorize_layer as a TextVectorization layer and building its vocabulary through the adaptation process on text_ds, it becomes a fundamental component as the initial layer in the end-to-end classification model. \n",
    "\n",
    "- This layer efficiently feeds transformed strings into the subsequent Embedding layer.\n",
    "\n",
    "- The `Embedding layer` takes the `integer-encoded vocabulary` and retrieves the corresponding `embedding vector` for each word-index. \n",
    "\n",
    "- These vectors evolve and improve as the model undergoes training, adding an extra dimension to the output array. The resultant dimensions following this operation are (batch, sequence, embedding).\n",
    "\n",
    "- To obtain a fixed-length output vector for each example, the model incorporates the GlobalAveragePooling1D layer. \n",
    "\n",
    "- This layer achieves this by averaging over the sequence dimension, ensuring the model can handle inputs of varying lengths in a straightforward manner.\n",
    "\n",
    "- The fixed-length output vector then progresses through a fully-connected (Dense) layer featuring 16 hidden units.\n",
    "\n",
    "- Concluding the architecture, the last layer establishes a dense connection with a single output node.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b70847aa-c098-45c8-aeb1-71681af858c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'), # optional\n",
    "  Dense(1)                      # binary\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb88a47-78f8-4c12-9e7b-8cb001678994",
   "metadata": {},
   "source": [
    "#### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4231943e-b485-49b6-b37a-b3172752a157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c28290ed-5be2-4805-ab96-ba8cd95eff70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ANACONDA\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "167e045a-1e70-4614-96ed-6437a50990d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 100)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160289 (626.13 KB)\n",
      "Trainable params: 160289 (626.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6eff1210-6b07-4f59-9c9f-f35234199d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From D:\\ANACONDA\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "20/20 [==============================] - 14s 648ms/step - loss: 0.6917 - accuracy: 0.5028 - val_loss: 0.6894 - val_accuracy: 0.4886\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 2s 108ms/step - loss: 0.6864 - accuracy: 0.5028 - val_loss: 0.6827 - val_accuracy: 0.4886\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 2s 114ms/step - loss: 0.6777 - accuracy: 0.5028 - val_loss: 0.6724 - val_accuracy: 0.4886\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 2s 117ms/step - loss: 0.6646 - accuracy: 0.5028 - val_loss: 0.6571 - val_accuracy: 0.4886\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 2s 110ms/step - loss: 0.6454 - accuracy: 0.5029 - val_loss: 0.6364 - val_accuracy: 0.4892\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 2s 116ms/step - loss: 0.6204 - accuracy: 0.5314 - val_loss: 0.6112 - val_accuracy: 0.5564\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 2s 111ms/step - loss: 0.5906 - accuracy: 0.6087 - val_loss: 0.5829 - val_accuracy: 0.6256\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 2s 107ms/step - loss: 0.5577 - accuracy: 0.6806 - val_loss: 0.5535 - val_accuracy: 0.6726\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 2s 118ms/step - loss: 0.5237 - accuracy: 0.7314 - val_loss: 0.5250 - val_accuracy: 0.7098\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 2s 108ms/step - loss: 0.4907 - accuracy: 0.7637 - val_loss: 0.4989 - val_accuracy: 0.7374\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 2s 113ms/step - loss: 0.4601 - accuracy: 0.7877 - val_loss: 0.4761 - val_accuracy: 0.7552\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 2s 111ms/step - loss: 0.4326 - accuracy: 0.8055 - val_loss: 0.4568 - val_accuracy: 0.7638\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 2s 115ms/step - loss: 0.4083 - accuracy: 0.8205 - val_loss: 0.4408 - val_accuracy: 0.7750\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 2s 108ms/step - loss: 0.3868 - accuracy: 0.8314 - val_loss: 0.4275 - val_accuracy: 0.7826\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 2s 115ms/step - loss: 0.3675 - accuracy: 0.8410 - val_loss: 0.4162 - val_accuracy: 0.7912\n",
      "CPU times: total: 23.3 s\n",
      "Wall time: 45.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28c72920950>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6538b547-d206-4e3a-96d7-171a4a8902c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #docs_infra: no_execute\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca3c79-b50a-479b-a73e-a7fd07695858",
   "metadata": {},
   "source": [
    "#### Retrieve the trained word embeddings and save them to disk\n",
    "Next, retrieve the word embeddings learned during training. \n",
    "\n",
    "The embeddings are weights of the Embedding layer in the model. The weights matrix is of shape (vocab_size, embedding_dimension).\n",
    "\n",
    "Obtain the weights from the model using get_layer() and get_weights(). \n",
    "\n",
    "The get_vocabulary() function provides the vocabulary to build a metadata file with one token per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6367a3f6-8982-4a01-a2e3-b283402134b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37aad583-4325-4439-b87d-e4b059af41f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c7256-e86f-4431-a3b1-e11a6f6b64fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
